{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QTa_vqhvV72k",
        "outputId": "dfcc35a2-d182-480d-f502-72b3e88674fb"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "OIn03nktTkOX"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/eri/anaconda3/envs/mrp/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import librosa\n",
        "import torch\n",
        "from transformers import WhisperProcessor, WhisperForConditionalGeneration\n",
        "from tqdm import tqdm\n",
        "from itertools import islice\n",
        "from pathlib import Path\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "LQmO2hhZVpel"
      },
      "outputs": [],
      "source": [
        "\n",
        "# ─── CONFIG ─────────────────────────────────────────────────────────────────────\n",
        "\n",
        "DATASET_ROOT = \"[DATASET_ROOT_PATH]\"\n",
        "DATASET_NAME = \"en\"\n",
        "\n",
        "# Path to dataset clips\n",
        "ROOT_CLIPS = os.path.join(DATASET_ROOT, DATASET_NAME, 'clips')\n",
        "\n",
        "# Path to your input TSV with columns:\n",
        "#    client_id, path, sentence, ... etc.\n",
        "INPUT_TSV = os.path.join(DATASET_ROOT, f\"{DATASET_NAME}/annotation/test.tsv\")\n",
        "# Root directory where audio files live\n",
        "AUDIO_ROOT = os.path.join(DATASET_ROOT, f\"{DATASET_NAME}/clips\")\n",
        "\n",
        "# Path where to write the output TSV\n",
        "SAVE_PATH = os.path.join(DATASET_ROOT, f\"{DATASET_NAME}/whisper-outputs\")\n",
        "os.makedirs(SAVE_PATH, exist_ok=True)\n",
        "OUTPUT_TSV = os.path.join(SAVE_PATH, f\"asr_output_test.tsv\")\n",
        "\n",
        "# Whisper model checkpoint\n",
        "WHISPER_MODEL = \"openai/whisper-tiny.en\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "1qxbOPoJVsOc"
      },
      "outputs": [],
      "source": [
        "\n",
        "# ─── MODULES ────────────────────────────────────────────────────────────────────\n",
        "\n",
        "def load_metadata(tsv_path: str) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Load the metadata TSV into a DataFrame.\n",
        "    Expects a 'path' column pointing to audio filenames,\n",
        "    and a 'sentence' column with the target text.\n",
        "    \"\"\"\n",
        "    df = pd.read_csv(tsv_path, sep=\"\\t\", dtype=str)\n",
        "    if \"path\" not in df.columns or \"sentence\" not in df.columns:\n",
        "        raise ValueError(\"Input TSV must contain 'path' and 'sentence' columns.\")\n",
        "    return df\n",
        "\n",
        "def init_asr_model(model_name: str):\n",
        "    \"\"\"\n",
        "    Load and return the Whisper processor & model.\n",
        "    \"\"\"\n",
        "    processor = WhisperProcessor.from_pretrained(model_name)\n",
        "    model     = WhisperForConditionalGeneration.from_pretrained(model_name)\n",
        "    return processor, model\n",
        "\n",
        "def transcribe_audio(\n",
        "    processor: WhisperProcessor,\n",
        "    model: WhisperForConditionalGeneration,\n",
        "    audio_path: str,\n",
        "    sr: int = 16_000,\n",
        ") -> str:\n",
        "    \"\"\"\n",
        "    Load an audio file, run Whisper ASR, and return the transcription string.\n",
        "    \"\"\"\n",
        "    # 1) Load with librosa at the expected sampling rate\n",
        "    wav, _ = librosa.load(audio_path, sr=sr)\n",
        "\n",
        "    # 2) Extract input features for Whisper\n",
        "    inputs = processor(\n",
        "        wav,\n",
        "        sampling_rate=sr,\n",
        "        return_tensors=\"pt\",\n",
        "    ).input_features.to(model.device)\n",
        "\n",
        "    # 3) Generate predicted token IDs\n",
        "    predicted_ids = model.generate(inputs)\n",
        "\n",
        "    # 4) Decode to text (keep special tokens if you like, skip otherwise)\n",
        "    transcription = processor.batch_decode(\n",
        "        predicted_ids,\n",
        "        skip_special_tokens=True\n",
        "    )[0]\n",
        "\n",
        "    return transcription\n",
        "\n",
        "def run_inference(\n",
        "    metadata_df: pd.DataFrame,\n",
        "    processor: WhisperProcessor,\n",
        "    model: WhisperForConditionalGeneration,\n",
        "    audio_root: str\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    For each row in metadata_df, transcribe the corresponding audio file.\n",
        "    Returns a new DataFrame with columns: [audio_name, target_text, transcription].\n",
        "    \"\"\"\n",
        "    results = []\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    for _, row in tqdm(metadata_df.iterrows(), total=len(metadata_df), desc=\"ASR Inference\"):\n",
        "        audio_name  = row[\"path\"]\n",
        "        target_text = row[\"sentence\"]\n",
        "        audio_path  = os.path.join(audio_root, audio_name)\n",
        "\n",
        "        if not os.path.isfile(audio_path):\n",
        "            # Skip missing files, or you could raise an error\n",
        "            transcription = \"<MISSING>\"\n",
        "        else:\n",
        "            transcription = transcribe_audio(processor, model, audio_path)\n",
        "\n",
        "        results.append({\n",
        "            \"audio_name\":     audio_name,\n",
        "            \"target_text\":    target_text,\n",
        "            \"transcription\":  transcription\n",
        "        })\n",
        "\n",
        "    return pd.DataFrame(results)\n",
        "\n",
        "\n",
        "def run_inference_batch(\n",
        "    metadata_df: pd.DataFrame,\n",
        "    processor: WhisperProcessor,\n",
        "    model: WhisperForConditionalGeneration,\n",
        "    audio_root: str,\n",
        "    batch_size: int = 1024,\n",
        "    sr: int = 16000\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Batch-process the metadata_df through Whisper.\n",
        "\n",
        "    Args:\n",
        "      metadata_df:   DataFrame with columns ['path','sentence']\n",
        "      processor:     WhisperProcessor\n",
        "      model:         WhisperForConditionalGeneration\n",
        "      audio_root:    root directory for all audio files\n",
        "      batch_size:    how many samples per batch\n",
        "      sr:            sampling rate for librosa.load\n",
        "\n",
        "    Returns:\n",
        "      DataFrame with columns ['audio_name','target_text','transcription']\n",
        "    \"\"\"\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    results = []\n",
        "    it = iter(metadata_df.to_dict('records'))\n",
        "\n",
        "    def batched_iterator(iterator, size):\n",
        "        \"\"\"Yield lists of up to `size` items from iterator.\"\"\"\n",
        "        while True:\n",
        "            batch = list(islice(iterator, size))\n",
        "            if not batch:\n",
        "                break\n",
        "            yield batch\n",
        "\n",
        "    for batch in tqdm(batched_iterator(it, batch_size), desc=\"ASR Batches\"):\n",
        "        # 1) Load all wavs in this batch\n",
        "        audio_names   = [row['path'] for row in batch]\n",
        "        target_texts  = [row['sentence'] for row in batch]\n",
        "        wavs = []\n",
        "        for name in audio_names:\n",
        "            full_path = os.path.join(audio_root, name)\n",
        "            if os.path.isfile(full_path):\n",
        "                wav, _ = librosa.load(full_path, sr=sr)\n",
        "            else:\n",
        "                wav = None\n",
        "            wavs.append(wav)\n",
        "\n",
        "        # 2) Prepare inputs — only for existing wavs\n",
        "        valid_indices = [i for i,w in enumerate(wavs) if w is not None]\n",
        "        valid_wavs    = [wavs[i] for i in valid_indices]\n",
        "\n",
        "        if valid_wavs:\n",
        "            inputs = processor(\n",
        "                valid_wavs,\n",
        "                sampling_rate=sr,\n",
        "                return_tensors=\"pt\",\n",
        "                padding=True\n",
        "            ).input_features.to(device)\n",
        "\n",
        "            # 3) Generate in one go\n",
        "            with torch.no_grad():\n",
        "                predicted_ids = model.generate(inputs)\n",
        "\n",
        "            # 4) Decode all\n",
        "            decoded = processor.batch_decode(predicted_ids, skip_special_tokens=True)\n",
        "\n",
        "        # 5) Collect results, preserving order\n",
        "        di = 0\n",
        "        for idx, name, tgt in zip(range(len(batch)), audio_names, target_texts):\n",
        "            if wavs[idx] is None:\n",
        "                transcription = \"<MISSING>\"\n",
        "            else:\n",
        "                transcription = decoded[di]\n",
        "                di += 1\n",
        "\n",
        "            results.append({\n",
        "                \"audio_name\":    name,\n",
        "                \"target_text\":   tgt,\n",
        "                \"transcription\": transcription\n",
        "            })\n",
        "\n",
        "    return pd.DataFrame(results)\n",
        "\n",
        "CHECKPOINT_PATH = \"partial_results.tsv\"\n",
        "CHECKPOINT_PERCENT = 5  # checkpoint every 5%\n",
        "\n",
        "def run_inference_with_checkpoint(\n",
        "    metadata_df: pd.DataFrame,\n",
        "    processor,\n",
        "    model,\n",
        "    audio_root: str,\n",
        "    checkpoint_path: str = CHECKPOINT_PATH,\n",
        "    checkpoint_percent: int = CHECKPOINT_PERCENT\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Exactly like run_inference(), but every `checkpoint_percent`% of the way through\n",
        "    it writes out a partial TSV so if you get killed you can resume.\n",
        "    \"\"\"\n",
        "\n",
        "    total = len(metadata_df)\n",
        "    interval = max(1, int(total * checkpoint_percent / 100))\n",
        "\n",
        "    # 1) See if we have an existing checkpoint to resume from\n",
        "    if os.path.exists(checkpoint_path):\n",
        "        done_df = pd.read_csv(checkpoint_path, sep=\"\\t\")\n",
        "        done_set = set(done_df[\"audio_name\"])\n",
        "        results = done_df.to_dict(\"records\")\n",
        "        start_idx = done_df.shape[0]\n",
        "        print(f\"🔄 Resuming from checkpoint: already have {start_idx}/{total}\")\n",
        "    else:\n",
        "        results = []\n",
        "        done_set = set()\n",
        "        start_idx = 0\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device).eval()\n",
        "\n",
        "    for i, (_, row) in enumerate(\n",
        "        tqdm(\n",
        "            list(metadata_df.iloc[start_idx:].iterrows()),\n",
        "            total=total - start_idx,\n",
        "            desc=\"ASR Inference (with ckpt)\"\n",
        "        ), start=start_idx\n",
        "    ):\n",
        "        audio_name  = row[\"path\"]\n",
        "        target_text = row[\"sentence\"]\n",
        "\n",
        "        if audio_name in done_set:\n",
        "            continue\n",
        "\n",
        "        audio_path = os.path.join(audio_root, audio_name)\n",
        "        if not os.path.isfile(audio_path):\n",
        "            transcription = \"<MISSING>\"\n",
        "        else:\n",
        "            transcription = transcribe_audio(processor, model, audio_path)\n",
        "\n",
        "        results.append({\n",
        "            \"audio_name\":     audio_name,\n",
        "            \"target_text\":    target_text,\n",
        "            \"transcription\":  transcription\n",
        "        })\n",
        "        done_set.add(audio_name)\n",
        "\n",
        "        # every `interval` items, write a checkpoint\n",
        "        if (i + 1) % interval == 0 or (i + 1) == total:\n",
        "            pd.DataFrame(results).to_csv(checkpoint_path, sep=\"\\t\", index=False)\n",
        "            print(f\"💾 Checkpoint at {i+1}/{total} rows\")\n",
        "\n",
        "    return pd.DataFrame(results)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import torch\n",
        "from itertools import islice\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "CHECKPOINT_PATH = \"partial_batch_results.tsv\"\n",
        "CHECKPOINT_PERCENT = 5  # checkpoint every 5%\n",
        "\n",
        "def run_inference_batch_with_checkpoint(\n",
        "    metadata_df: pd.DataFrame,\n",
        "    processor,\n",
        "    model,\n",
        "    audio_root: str,\n",
        "    batch_size: int = 64,\n",
        "    sr: int = 16000,\n",
        "    checkpoint_path: str = CHECKPOINT_PATH,\n",
        "    checkpoint_percent: int = CHECKPOINT_PERCENT\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Batch-process with Whisper, checkpointing every `checkpoint_percent`% of the total rows.\n",
        "    On startup, resumes from any existing `checkpoint_path`.\n",
        "    \"\"\"\n",
        "    # Prepare records and checkpoint\n",
        "    records = metadata_df.to_dict(\"records\")\n",
        "    total = len(records)\n",
        "    interval = max(1, int(total * checkpoint_percent / 100))\n",
        "\n",
        "    if os.path.exists(checkpoint_path):\n",
        "        done_df = pd.read_csv(checkpoint_path, sep=\"\\t\")\n",
        "        done_set = set(done_df[\"audio_name\"])\n",
        "        results = done_df.to_dict(\"records\")\n",
        "        start_idx = done_df.shape[0]\n",
        "        print(f\"🔄 Resuming from checkpoint: {start_idx}/{total} rows done\")\n",
        "    else:\n",
        "        done_set = set()\n",
        "        results = []\n",
        "        start_idx = 0\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device).eval()\n",
        "\n",
        "    # Process in batches, but only from start_idx onward\n",
        "    for batch_start in range(start_idx, total, batch_size):\n",
        "        batch = records[batch_start : batch_start + batch_size]\n",
        "\n",
        "        # 1) Load wavs\n",
        "        wavs = []\n",
        "        for rec in batch:\n",
        "            name = rec[\"path\"]\n",
        "            path = os.path.join(audio_root, name)\n",
        "            if os.path.isfile(path):\n",
        "                wav, _ = librosa.load(path, sr=sr)\n",
        "            else:\n",
        "                wav = None\n",
        "            wavs.append(wav)\n",
        "\n",
        "        # 2) Prepare & run model on valid wavs\n",
        "        valid_indices = [i for i, w in enumerate(wavs) if w is not None]\n",
        "        decoded = []\n",
        "        if valid_indices:\n",
        "            valid_wavs = [wavs[i] for i in valid_indices]\n",
        "            inputs = processor(\n",
        "                valid_wavs,\n",
        "                sampling_rate=sr,\n",
        "                return_tensors=\"pt\",\n",
        "                padding=True\n",
        "            ).input_features.to(device)\n",
        "            with torch.no_grad():\n",
        "                preds = model.generate(inputs)\n",
        "            decoded = processor.batch_decode(preds, skip_special_tokens=True)\n",
        "\n",
        "        # 3) Collect results, use absolute index for checkpoint logic\n",
        "        di = 0\n",
        "        for offset, rec in enumerate(batch):\n",
        "            abs_idx = batch_start + offset\n",
        "            name = rec[\"path\"]\n",
        "            target = rec[\"sentence\"]\n",
        "            if name in done_set:\n",
        "                continue\n",
        "\n",
        "            if wavs[offset] is None:\n",
        "                text = \"<MISSING>\"\n",
        "            else:\n",
        "                text = decoded[di]\n",
        "                di += 1\n",
        "\n",
        "            results.append({\n",
        "                \"audio_name\":    name,\n",
        "                \"target_text\":   target,\n",
        "                \"transcription\": text\n",
        "            })\n",
        "            done_set.add(name)\n",
        "\n",
        "            # checkpoint every interval rows, or at the very end\n",
        "            if (abs_idx + 1) % interval == 0 or (abs_idx + 1) == total:\n",
        "                pd.DataFrame(results).to_csv(checkpoint_path, sep=\"\\t\", index=False)\n",
        "                print(f\"💾 Checkpoint at {abs_idx + 1}/{total}\")\n",
        "\n",
        "    return pd.DataFrame(results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "GYqeUJ0-XLN8"
      },
      "outputs": [],
      "source": [
        "meta_df = load_metadata(INPUT_TSV)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 276
        },
        "id": "28YYKhghXMUz",
        "outputId": "927ccf34-62b3-4927-842b-f704bfb00d74"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>client_id</th>\n",
              "      <th>path</th>\n",
              "      <th>sentence</th>\n",
              "      <th>up_votes</th>\n",
              "      <th>down_votes</th>\n",
              "      <th>age</th>\n",
              "      <th>gender</th>\n",
              "      <th>accent</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0013037a1d45cc33460806cc3f8ecee9d536c45639ba4c...</td>\n",
              "      <td>common_voice_en_699711.mp3</td>\n",
              "      <td>She'll be all right.</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>001509f4624a7dee75247f6a8b642c4a0d09f8be3eeea6...</td>\n",
              "      <td>common_voice_en_18132047.mp3</td>\n",
              "      <td>All's well that ends well.</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>003fb666a99eb3aa3ba05d9c8641c18e55cf7d34d1b981...</td>\n",
              "      <td>common_voice_en_17263741.mp3</td>\n",
              "      <td>Do you mean it?</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>004017ba82a23768d58dff3b91da8e8f951ea5fb6d3cd9...</td>\n",
              "      <td>common_voice_en_17893917.mp3</td>\n",
              "      <td>The new patch is less invasive than the old on...</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0047f1aea3f39c4c6a9298d84f046c1f84f439f594d840...</td>\n",
              "      <td>common_voice_en_17561821.mp3</td>\n",
              "      <td>How is Mozilla going to handle ambiguities lik...</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                           client_id  \\\n",
              "0  0013037a1d45cc33460806cc3f8ecee9d536c45639ba4c...   \n",
              "1  001509f4624a7dee75247f6a8b642c4a0d09f8be3eeea6...   \n",
              "2  003fb666a99eb3aa3ba05d9c8641c18e55cf7d34d1b981...   \n",
              "3  004017ba82a23768d58dff3b91da8e8f951ea5fb6d3cd9...   \n",
              "4  0047f1aea3f39c4c6a9298d84f046c1f84f439f594d840...   \n",
              "\n",
              "                           path  \\\n",
              "0    common_voice_en_699711.mp3   \n",
              "1  common_voice_en_18132047.mp3   \n",
              "2  common_voice_en_17263741.mp3   \n",
              "3  common_voice_en_17893917.mp3   \n",
              "4  common_voice_en_17561821.mp3   \n",
              "\n",
              "                                            sentence up_votes down_votes  age  \\\n",
              "0                               She'll be all right.        2          1  NaN   \n",
              "1                         All's well that ends well.        2          0  NaN   \n",
              "2                                    Do you mean it?        2          0  NaN   \n",
              "3  The new patch is less invasive than the old on...        2          1  NaN   \n",
              "4  How is Mozilla going to handle ambiguities lik...        2          0  NaN   \n",
              "\n",
              "  gender accent  \n",
              "0    NaN    NaN  \n",
              "1    NaN    NaN  \n",
              "2    NaN    NaN  \n",
              "3    NaN    NaN  \n",
              "4    NaN    NaN  "
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "meta_df.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Find and Remove Faulty Audios\n",
        "def remove_faulty_samples(df, root_clips=\"[Path_to_ASR_DATASET_AUDIO_CLIPS]\", \n",
        "    ):\n",
        "    audio_dir = Path(root_clips)\n",
        "\n",
        "    # Find problematic files\n",
        "    bad_files = []\n",
        "    for mp3_path in audio_dir.glob(\"*.mp3\"):\n",
        "        if not mp3_path.exists():\n",
        "            print(f\"Missing file: {mp3_path}\")\n",
        "            continue\n",
        "        if mp3_path.stat().st_size < 1024:  # Check for empty/small files\n",
        "            print(f\"Corrupted file: {mp3_path}\")\n",
        "            bad_files.append(mp3_path)\n",
        "    \n",
        "    # Assuming badfiles contains full paths to faulty audio files\n",
        "    # Extract just the filenames from badfiles paths\n",
        "    faulty_filenames = {os.path.basename(badfile) for badfile in bad_files}\n",
        "\n",
        "    # Filter the dataframe to find matching entries\n",
        "    faulty_samples = meta_df[meta_df['path'].isin(faulty_filenames)]\n",
        "\n",
        "    # Print results\n",
        "    print(f\"Found {len(faulty_samples)} faulty audio samples:\")\n",
        "    for idx, row in faulty_samples.iterrows():\n",
        "        print(f\"- {row['path']} (Index: {idx})\")\n",
        "    \n",
        "    # Get clean DataFrame (excluding faulty samples)\n",
        "    clean_meta_df = meta_df[~meta_df['path'].isin(faulty_filenames)]\n",
        "\n",
        "    # Verify removal\n",
        "    print(f\"Original rows: {len(meta_df)}\")\n",
        "    print(f\"Clean rows: {len(clean_meta_df)}\")\n",
        "    print(f\"Removed {len(meta_df) - len(clean_meta_df)} faulty samples\")\n",
        "        \n",
        "    return clean_meta_df\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oXaeNnmMVwHr",
        "outputId": "286a5e34-2a71-4deb-8c0c-72fdc453c892"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Corrupted file: [YOUR_ROOT_PATH]/datasets/audio-dataset/en/clips/common_voice_en_37101.mp3\n",
            "Corrupted file: [YOUR_ROOT_PATH]/datasets/audio-dataset/en/clips/common_voice_en_626040.mp3\n",
            "Corrupted file: [YOUR_ROOT_PATH]/datasets/audio-dataset/en/clips/common_voice_en_577779.mp3\n",
            "Corrupted file: [YOUR_ROOT_PATH]/datasets/audio-dataset/en/clips/common_voice_en_641439.mp3\n",
            "Found 0 faulty audio samples:\n",
            "Original rows: 15531\n",
            "Clean rows: 15531\n",
            "Removed 0 faulty samples\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask is not set and cannot be inferred from input because pad token is same as eos token.As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "💾 Checkpoint at 776/15531\n",
            "💾 Checkpoint at 1552/15531\n",
            "💾 Checkpoint at 2328/15531\n",
            "💾 Checkpoint at 3104/15531\n",
            "💾 Checkpoint at 3880/15531\n",
            "💾 Checkpoint at 4656/15531\n",
            "💾 Checkpoint at 5432/15531\n",
            "💾 Checkpoint at 6208/15531\n",
            "💾 Checkpoint at 6984/15531\n",
            "💾 Checkpoint at 7760/15531\n",
            "💾 Checkpoint at 8536/15531\n",
            "💾 Checkpoint at 9312/15531\n",
            "💾 Checkpoint at 10088/15531\n",
            "💾 Checkpoint at 10864/15531\n",
            "💾 Checkpoint at 11640/15531\n",
            "💾 Checkpoint at 12416/15531\n",
            "💾 Checkpoint at 13192/15531\n",
            "💾 Checkpoint at 13968/15531\n",
            "💾 Checkpoint at 14744/15531\n",
            "💾 Checkpoint at 15520/15531\n",
            "💾 Checkpoint at 15531/15531\n",
            "✅ Wrote 15531 lines to [YOUR_ROOT_PATH]/datasets/audio-dataset/en/whisper-outputs/asr_output_test.tsv\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# ─── MAIN ENTRYPOINT ─────────────────────────────────────────────────────────────\n",
        "\n",
        "def main_old_old():\n",
        "    # 1) Load metadata\n",
        "    meta_df = load_metadata(INPUT_TSV)\n",
        "\n",
        "    # 2) Init ASR model\n",
        "    processor, model = init_asr_model(WHISPER_MODEL)\n",
        "\n",
        "    # 3) Run inference\n",
        "    out_df = run_inference(meta_df, processor, model, AUDIO_ROOT)\n",
        "\n",
        "    # 4) Save to TSV\n",
        "    out_df.to_csv(OUTPUT_TSV, sep=\"\\t\", index=False)\n",
        "    print(f\"✅ Wrote {len(out_df)} lines to {OUTPUT_TSV}\")\n",
        "\n",
        "\n",
        "def main_old():\n",
        "    # 1) Load metadata\n",
        "    meta_df = load_metadata(INPUT_TSV)\n",
        "\n",
        "    # 2) Init ASR model\n",
        "    processor, model = init_asr_model(WHISPER_MODEL)\n",
        "\n",
        "    # 3) Run inference with checkpointing\n",
        "    out_df = run_inference_with_checkpoint(meta_df, processor, model, AUDIO_ROOT)\n",
        "\n",
        "    # 4) Save final TSV\n",
        "    out_df.to_csv(OUTPUT_TSV, sep=\"\\t\", index=False)\n",
        "    print(f\"✅ Wrote {len(out_df)} lines to {OUTPUT_TSV}\")\n",
        "\n",
        "def main():\n",
        "    meta_df   = load_metadata(INPUT_TSV)\n",
        "    meta_df   = remove_faulty_samples(meta_df, root_clips=AUDIO_ROOT)\n",
        "    processor, model = init_asr_model(WHISPER_MODEL)\n",
        "    out_df    = run_inference_batch_with_checkpoint(\n",
        "                    meta_df,\n",
        "                    processor,\n",
        "                    model,\n",
        "                    AUDIO_ROOT,\n",
        "                    batch_size=8,\n",
        "                    sr=16_000\n",
        "                )\n",
        "    out_df.to_csv(OUTPUT_TSV, sep=\"\\t\", index=False)\n",
        "    print(f\"✅ Wrote {len(out_df)} lines to {OUTPUT_TSV}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rHqFzS8ygydj"
      },
      "source": [
        "# Evaluate and Align"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Filter Faulty Samples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Filtered row 203:\n",
            "  Audio: common_voice_en_479944.mp3\n",
            "  Target: 'But instead of being saddened, he was happy.'\n",
            "  Transcription: ' .'\n",
            "--------------------------------------------------\n",
            "Filtered row 255:\n",
            "  Audio: common_voice_en_509774.mp3\n",
            "  Target: \"You're just the one I wanted to see.\"\n",
            "  Transcription: ' .'\n",
            "--------------------------------------------------\n",
            "Filtered row 459:\n",
            "  Audio: common_voice_en_16759015.mp3\n",
            "  Target: ''\n",
            "  Transcription: ' HTML when it grows in'\n",
            "--------------------------------------------------\n",
            "Filtered row 1295:\n",
            "  Audio: common_voice_en_17846037.mp3\n",
            "  Target: 'First impressions are the most lasting.'\n",
            "  Transcription: ' .'\n",
            "--------------------------------------------------\n",
            "Filtered row 1353:\n",
            "  Audio: common_voice_en_696712.mp3\n",
            "  Target: 'A young Arab, also loaded down with baggage, entered, and greeted the Englishman.'\n",
            "  Transcription: ' .'\n",
            "--------------------------------------------------\n",
            "Filtered row 1464:\n",
            "  Audio: common_voice_en_503476.mp3\n",
            "  Target: 'But I didn’t think that it contained any living creature.'\n",
            "  Transcription: ' .'\n",
            "--------------------------------------------------\n",
            "Filtered row 2040:\n",
            "  Audio: common_voice_en_675386.mp3\n",
            "  Target: 'I have come to enquire about the antelope.'\n",
            "  Transcription: ' .'\n",
            "--------------------------------------------------\n",
            "Filtered row 2265:\n",
            "  Audio: common_voice_en_18456071.mp3\n",
            "  Target: 'These are a chest of drawers.'\n",
            "  Transcription: ' .'\n",
            "--------------------------------------------------\n",
            "Filtered row 11788:\n",
            "  Audio: common_voice_en_529817.mp3\n",
            "  Target: 'What the dickens'\n",
            "  Transcription: ' .'\n",
            "--------------------------------------------------\n",
            "\n",
            "Filtering complete:\n",
            "  Total rows: 15531\n",
            "  Valid rows: 15522\n",
            "  Filtered rows: 9\n",
            "  Clean TSV saved to: [YOUR_ROOT_PATH]/datasets/audio-dataset/en/whisper-outputs/filtered_asr_output_test.tsv\n"
          ]
        }
      ],
      "source": [
        "import csv\n",
        "import re\n",
        "\n",
        "def normalize_text(text):\n",
        "    \"\"\"Robust text normalization handling float/None\"\"\"\n",
        "    if text is None or isinstance(text, float):\n",
        "        return \"\"\n",
        "    if not isinstance(text, str):\n",
        "        try:\n",
        "            text = str(text)\n",
        "        except:\n",
        "            return \"\"\n",
        "    \n",
        "    # Normalization steps\n",
        "    text = text.lower().strip()\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n",
        "    text = re.sub(r'\\s+', ' ', text)      # Normalize whitespace\n",
        "    return text\n",
        "\n",
        "def filter_faulty_rows(input_tsv, output_tsv):\n",
        "    \"\"\"Filter rows with invalid text fields\"\"\"\n",
        "    valid_rows = 0\n",
        "    total_rows = 0\n",
        "    filtered_rows = 0\n",
        "    \n",
        "    with open(input_tsv, 'r') as infile, open(output_tsv, 'w') as outfile:\n",
        "        reader = csv.DictReader(infile, delimiter='\\t')\n",
        "        writer = csv.DictWriter(outfile, fieldnames=reader.fieldnames, delimiter='\\t')\n",
        "        writer.writeheader()\n",
        "        \n",
        "        for total_rows, row in enumerate(reader, 1):\n",
        "            target = row.get('target_text', '')\n",
        "            trans = row.get('transcription', '')\n",
        "            \n",
        "            # Normalize and check validity\n",
        "            norm_target = normalize_text(target)\n",
        "            norm_trans = normalize_text(trans)\n",
        "            \n",
        "            # Filter criteria\n",
        "            is_valid = (\n",
        "                isinstance(target, str) and \n",
        "                isinstance(trans, str) and\n",
        "                norm_target != \"\" and\n",
        "                norm_trans != \"\"\n",
        "            )\n",
        "            \n",
        "            if is_valid:\n",
        "                writer.writerow(row)\n",
        "                valid_rows += 1\n",
        "            else:\n",
        "                filtered_rows += 1\n",
        "                print(f\"Filtered row {total_rows}:\")\n",
        "                print(f\"  Audio: {row.get('audio_name', '')}\")\n",
        "                print(f\"  Target: {repr(target)}\")\n",
        "                print(f\"  Transcription: {repr(trans)}\")\n",
        "                print(\"-\" * 50)\n",
        "    \n",
        "    print(\"\\nFiltering complete:\")\n",
        "    print(f\"  Total rows: {total_rows}\")\n",
        "    print(f\"  Valid rows: {valid_rows}\")\n",
        "    print(f\"  Filtered rows: {filtered_rows}\")\n",
        "    print(f\"  Clean TSV saved to: {output_tsv}\")\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    input_tsv = \"[Path_to_your_asr_inference_annotation]\" #includes audio path and target text\n", 
        "    output_tsv = \"[Output_Path]\" # a TSV file\n",
        "    filter_faulty_rows(input_tsv, output_tsv)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uL3aWwpUhinK",
        "outputId": "f4d64c6c-7b63-4ef5-9012-df20a7804e12"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overall WER = \n",
            "Overall CER = \n",
            "Wrote detailed results to filtered_asr_output_test.tsv\n"
          ]
        }
      ],
      "source": [
        "!python data_preprocess.py \"[Output_Path]\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V28",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "mrp",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
